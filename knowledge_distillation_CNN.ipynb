{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eac2565-60b5-4aca-ae50-d26a05076ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Ref https://arxiv.org/pdf/1503.02531\n",
    "# https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "# Check if the current `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# is available, and if not, use the CPU\n",
    "device ='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "#%%\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='../data', train=False, download=True, transform=transforms_cifar)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=1)\n",
    "#%%\n",
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Conv2d(3, 256, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.utils.weight_norm(nn.Conv2d(256, 256, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.utils.weight_norm(nn.Conv2d(256, 128, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.utils.weight_norm(nn.Conv2d(128, 128, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.utils.weight_norm(nn.Conv2d(128, 64, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.utils.weight_norm(nn.Conv2d(64, 64, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),            \n",
    "\n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(64, 32, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.utils.weight_norm(nn.Conv2d(32, 32, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(128, 32)),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    def get_features(self, x):\n",
    "        return self.features(x)  # Final feature layer: [batch_size, 32, 2, 2]\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.raw_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.features = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Conv2d(3, 16, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(16, 16, kernel_size=3, padding=1)),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(1024, 256)),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Transform raw_weight to [0, 1] with sigmoid\n",
    "        soft_target_loss_weight = torch.sigmoid(self.raw_weight)\n",
    "        ce_loss_weight = 1.0 - soft_target_loss_weight  # Ensures sum = 1\n",
    "        return soft_target_loss_weight, ce_loss_weight\n",
    "\n",
    "    def get_features(self, x):\n",
    "        return self.features(x)  # Final feature layer: [batch_size, 32, 2, 2]\n",
    "    \n",
    "\n",
    "    #%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8973d9-68b6-4e29-a865-1ce62f749ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a32b54f-7153-487c-851d-c1acef28c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha_qfp58yg\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.236021380900117\n",
      "Epoch 2/10, Loss: 0.7769130855570059\n",
      "Epoch 3/10, Loss: 0.6030806303024292\n",
      "Epoch 4/10, Loss: 0.4924271858256796\n",
      "Epoch 5/10, Loss: 0.40683794940066764\n",
      "Epoch 6/10, Loss: 0.33695203138281926\n",
      "Epoch 7/10, Loss: 0.273998005939719\n",
      "Epoch 8/10, Loss: 0.21952090950687522\n",
      "Epoch 9/10, Loss: 0.1824772774868304\n",
      "Epoch 10/10, Loss: 0.14743810997861426\n",
      "Test Accuracy: 83.84%\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648935fd-3038-4804-ae0c-26bcdc96e4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,185,674\n",
      "LightNN parameters: 268,091\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)\n",
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff112c6-1594-4f89-a619-b1027551231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2836217776588772\n",
      "Epoch 2/10, Loss: 0.9650801682411252\n",
      "Epoch 3/10, Loss: 0.8392431889958394\n",
      "Epoch 4/10, Loss: 0.7397911864168504\n",
      "Epoch 5/10, Loss: 0.6573385456791314\n",
      "Epoch 6/10, Loss: 0.5813811342886952\n",
      "Epoch 7/10, Loss: 0.5148340121407033\n",
      "Epoch 8/10, Loss: 0.45092836487323734\n",
      "Epoch 9/10, Loss: 0.3894237912524387\n",
      "Epoch 10/10, Loss: 0.3307381728497308\n",
      "Test Accuracy: 72.35%\n"
     ]
    }
   ],
   "source": [
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd67063-1db4-4765-a0b2-43c5448df0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 83.84%\n",
      "Student accuracy: 72.35%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a0f595-2d40-42f0-92e9-882b551d8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,185,674\n",
      "LightNN parameters: 268,091\n"
     ]
    }
   ],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13506ac1-15df-436b-86b8-3170f2775fa8",
   "metadata": {},
   "source": [
    "# Matching Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1995f4-7e15-42e0-9839-f16571ea9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "## trainable weights\n",
    "def train_knowledge_distillation_logits(teacher, student, train_loader, epochs, learning_rate, T,ce_loss_weight,soft_target_loss_weight, device):\n",
    "    # Define loss functions\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')  # KL Divergence loss with batch mean reduction\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    teacher.eval()  # Teacher in evaluation mode\n",
    "    student.train() # Student in training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with teacher model (no gradients)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Calculate softened probabilities\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate KL Divergence loss (distillation loss), scaled by T**2\n",
    "            soft_targets_loss = kd_loss(soft_prob, soft_targets) * (T ** 2)\n",
    "\n",
    "            # Calculate cross-entropy loss with true labels\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "            \n",
    "            # soft_target_loss_weight, ce_loss_weight = student.get_weights()\n",
    "\n",
    "            # Weighted combination of losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2babd6a6-fa7c-45ee-a712-ab89945328a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1825510928088137\n",
      "Epoch 2/10, Loss: 1.009930544497107\n",
      "Epoch 3/10, Loss: 0.939848689167091\n",
      "Epoch 4/10, Loss: 0.8905868812290298\n",
      "Epoch 5/10, Loss: 0.8520150484941195\n",
      "Epoch 6/10, Loss: 0.8196287685647949\n",
      "Epoch 7/10, Loss: 0.7913297737955742\n",
      "Epoch 8/10, Loss: 0.7637388416568337\n",
      "Epoch 9/10, Loss: 0.7414099674700471\n",
      "Epoch 10/10, Loss: 0.7162998509224113\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = DeepNN(num_classes=10).to(device)\n",
    "student = LightNN(num_classes=10).to(device)\n",
    "\n",
    "train_knowledge_distillation_logits(\n",
    "    teacher= teacher, \n",
    "    student= student, \n",
    "    train_loader=train_loader, \n",
    "    epochs=10, \n",
    "    learning_rate=0.001, \n",
    "    T=3, \n",
    "    soft_target_loss_weight=0.25,\n",
    "    ce_loss_weight = 0.75,\n",
    "    \n",
    "\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6adfee34-33b0-49f9-8508-0b7a9de2e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.54%\n",
      "Teacher accuracy: 83.84%\n",
      "Student accuracy without teacher: 72.35%\n",
      "Student accuracy with CE + KD: 74.54%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_light_ce_and_kd = test(student, test_loader, device)\n",
    "# Compare accuracies\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063ac22-7fa8-483d-8272-5f6564c9222d",
   "metadata": {},
   "source": [
    "# Gradient Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16b8062-f835-4a39-9840-1e099aaf0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#DOWNSAMPLE CODE GRADIENT\n",
    "# Compute gradient-based attention map\n",
    "def compute_gradient_attention(features, logits, target, retain_graph=False):\n",
    "    features.requires_grad_(True)\n",
    "    # .requires_grad is read only , .requires_grad_ is inplace operation\n",
    "    features.retain_grad()  # Retain gradients for non-leaf tensor\n",
    "    loss = nn.CrossEntropyLoss()(logits, target)\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    grad = features.grad  # [batch_size, channels, height, width]\n",
    "    attention = torch.mean(torch.abs(grad), dim=1, keepdim=True)  # [batch_size, 1, height, width]\n",
    "    features.grad = None  # Clear gradients\n",
    "    return attention\n",
    "\n",
    "def train_attention_transfer_grad(teacher, student, train_loader, epochs, learning_rate,attention_loss_weight, ce_loss_weight,device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    attention_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Downsampling layer to resize student's attention map from 8x8 to 2x2\n",
    "    downsample = nn.AvgPool2d(kernel_size=4, stride=4).to(device)  # 8x8 -> 2x2\n",
    "\n",
    "\n",
    "    teacher.eval()  # Teacher weights frozen\n",
    "    student.train() # Student trainable\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Teacher: Final feature layer\n",
    "            teacher_features = teacher.get_features(inputs)  # [batch_size, 32, 2, 2]\n",
    "            teacher_flat = torch.flatten(teacher_features, 1)  # [batch_size, 128]\n",
    "            teacher_logits = teacher.classifier(teacher_flat)  # [batch_size, 10]\n",
    "            teacher_attention = compute_gradient_attention(teacher_features, teacher_logits, labels)  # [batch_size, 1, 2, 2]\n",
    "\n",
    "            # Student: Final feature layer\n",
    "            student_features = student.get_features(inputs)  # [batch_size, 16, 8, 8]\n",
    "            student_flat = torch.flatten(student_features, 1)  # [batch_size, 1024]\n",
    "            student_logits = student.classifier(student_flat)  # [batch_size, 10]\n",
    "            student_attention = compute_gradient_attention(student_features, student_logits, labels, retain_graph=True)  # [batch_size, 1, 8, 8]\n",
    "            student_attention_downsampled = downsample(student_attention)  # [batch_size, 1, 2, 2]\n",
    "\n",
    "            # Attention transfer loss (no projection, direct comparison after downsampling)\n",
    "            attention_transfer_loss = attention_loss(student_attention_downsampled, teacher_attention.detach())\n",
    "\n",
    "            # Classification loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = attention_loss_weight * attention_transfer_loss + ce_loss_weight * label_loss \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9729c932-8075-46a3-bab4-e264139b6e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9751253135673835\n",
      "Epoch 2/10, Loss: 0.7323152788764681\n",
      "Epoch 3/10, Loss: 0.6286294257549374\n",
      "Epoch 4/10, Loss: 0.5500331466917492\n",
      "Epoch 5/10, Loss: 0.48883708869404807\n",
      "Epoch 6/10, Loss: 0.43106334113403966\n",
      "Epoch 7/10, Loss: 0.37476974245532396\n",
      "Epoch 8/10, Loss: 0.3264486915848749\n",
      "Epoch 9/10, Loss: 0.27466102138809534\n",
      "Epoch 10/10, Loss: 0.2334660065486608\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = DeepNN(num_classes=10).to(device)\n",
    "student = LightNN(num_classes=10).to(device)\n",
    "\n",
    "train_attention_transfer_grad(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    train_loader=train_loader,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    ce_loss_weight=0.75,\n",
    "    attention_loss_weight=0.25,\n",
    "\n",
    "\n",
    "    device=device\n",
    ")\n",
    "# Assuming test() function and test_accuracy_deep are defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdffa132-4415-48bf-bf72-e65a13c8362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.50%\n",
      "Teacher accuracy: 83.84%\n",
      "Student accuracy without teacher: 72.35%\n",
      "Student accuracy with CE + KD: 74.54%\n",
      "Student accuracy with CE + Grad Attention: 72.50%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_light_ce_and_grad = test(student, test_loader, device)\n",
    "\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + Grad Attention: {test_accuracy_light_ce_and_grad:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980d09a-3c0c-4514-b076-9f1f8bbdc1e6",
   "metadata": {},
   "source": [
    "# Feature Map Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c623b9d5-498e-4a52-b54a-30c56bb69a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION CODE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Compute squared attention map (based on squared activations)\n",
    "def compute_squared_attention(features):\n",
    "    # features: [batch_size, channels, height, width]\n",
    "    squared_features = features ** 2  # Square the activations\n",
    "    attention = torch.mean(squared_features, dim=1, keepdim=True)  # Mean across channels: [batch_size, 1, height, width]\n",
    "    return attention\n",
    "\n",
    "def train_attention_transfer(teacher, student, train_loader, epochs, learning_rate, attention_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    attention_loss = nn.MSELoss()  # Using MSE for squared attention loss\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Downsampling layer to resize student's attention map from 8x8 to 2x2\n",
    "    downsample = nn.AvgPool2d(kernel_size=4, stride=4).to(device)  # 8x8 -> 2x2\n",
    "\n",
    "    teacher.eval()  # Teacher weights frozen\n",
    "    student.train() # Student trainable\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Teacher: Final feature layer\n",
    "            teacher_features = teacher.get_features(inputs)  # [batch_size, 32, 2, 2]\n",
    "            teacher_attention = compute_squared_attention(teacher_features)  # [batch_size, 1, 2, 2]\n",
    "            teacher_flat = torch.flatten(teacher_features, 1)  # [batch_size, 128]\n",
    "            teacher_logits = teacher.classifier(teacher_flat)  # [batch_size, 10]\n",
    "\n",
    "            # Student: Final feature layer\n",
    "            student_features = student.get_features(inputs)  # [batch_size, 16, 8, 8]\n",
    "            student_attention = compute_squared_attention(student_features)  # [batch_size, 1, 8, 8]\n",
    "            student_attention_downsampled = downsample(student_attention)  # [batch_size, 1, 2, 2]\n",
    "            student_flat = torch.flatten(student_features, 1)  # [batch_size, 1024]\n",
    "            student_logits = student.classifier(student_flat)  # [batch_size, 10]\n",
    "\n",
    "            # Attention transfer loss (squared attention comparison)\n",
    "            attention_transfer_loss = attention_loss(student_attention_downsampled, teacher_attention.detach())\n",
    "\n",
    "            # Classification loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = attention_loss_weight * attention_transfer_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2188871f-7b29-499b-849d-10e12266b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1784591116868626\n",
      "Epoch 2/10, Loss: 0.849904999556139\n",
      "Epoch 3/10, Loss: 0.7473934905608292\n",
      "Epoch 4/10, Loss: 0.6853186504157913\n",
      "Epoch 5/10, Loss: 0.6368690300018282\n",
      "Epoch 6/10, Loss: 0.5985646842385802\n",
      "Epoch 7/10, Loss: 0.5621726176013118\n",
      "Epoch 8/10, Loss: 0.5301408989502646\n",
      "Epoch 9/10, Loss: 0.503132668983601\n",
      "Epoch 10/10, Loss: 0.4742340642168089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = DeepNN(num_classes=10).to(device)\n",
    "student = LightNN(num_classes=10).to(device)\n",
    "\n",
    "\n",
    "train_attention_transfer(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    train_loader=train_loader,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    attention_loss_weight=0.25,\n",
    "    ce_loss_weight=0.75,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870348bd-6b5d-438f-969e-dcb008f12abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.69%\n",
      "Teacher accuracy: 83.84%\n",
      "Student accuracy without teacher: 72.35%\n",
      "Student accuracy with CE + KD: 74.54%\n",
      "Student accuracy with CE + Grad Attention: 72.50%\n",
      "Student accuracy with CE + Squared Attention: 72.69%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_accuracy_light_ce_and_feat = test(student, test_loader, device)\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + Grad Attention: {test_accuracy_light_ce_and_grad:.2f}%\")\n",
    "print(f\"Student accuracy with CE + Squared Attention: {test_accuracy_light_ce_and_feat:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4c462-110d-43e4-98fb-61f67460d0ca",
   "metadata": {},
   "source": [
    "# Logit and Gradient Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05680d8d-7741-46ce-9d58-062d06075a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f609649-7163-43e9-b5f5-fd0d4eaf8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Compute gradient-based attention map\n",
    "def compute_gradient_attention(features, logits, target, retain_graph=False):\n",
    "    features.requires_grad_(True)\n",
    "    features.retain_grad()  # Retain gradients for non-leaf tensor\n",
    "    loss = nn.CrossEntropyLoss()(logits, target)\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    grad = features.grad  # [batch_size, channels, height, width]\n",
    "    attention = torch.mean(torch.abs(grad), dim=1, keepdim=True)  # [batch_size, 1, height, width]\n",
    "    features.grad = None  # Clear gradients\n",
    "    return attention\n",
    "\n",
    "def train_attention_transfer(teacher, student, train_loader, epochs, learning_rate,  T,attention_loss_weight, ce_loss_weight,soft_target_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    attention_loss = nn.MSELoss()\n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')  # KL Divergence loss with batch mean reduction\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Downsampling layer to resize student's attention map from 8x8 to 2x2\n",
    "    downsample = nn.AvgPool2d(kernel_size=4, stride=4).to(device)  # 8x8 -> \n",
    "\n",
    "\n",
    "    teacher.eval()  # Teacher weights frozen\n",
    "    student.train()  # Student trainable\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Teacher: Final feature layer\n",
    "            teacher_features = teacher.get_features(inputs)  # [batch_size, 32, 2, 2]\n",
    "            teacher_flat = torch.flatten(teacher_features, 1)  # [batch_size, 128]\n",
    "            teacher_logits = teacher.classifier(teacher_flat)  # [batch_size, 10]\n",
    "            teacher_attention = compute_gradient_attention(teacher_features, teacher_logits, labels)  # [batch_size, 1, 2, 2]\n",
    "\n",
    "            # Student: Final feature layer\n",
    "            student_features = student.get_features(inputs)  # [batch_size, 16, 8, 8]\n",
    "            student_flat = torch.flatten(student_features, 1)  # [batch_size, 1024]\n",
    "            student_logits = student.classifier(student_flat)  # [batch_size, 10]\n",
    "            student_attention = compute_gradient_attention(student_features, student_logits, labels, retain_graph=True)  # [batch_size, 1, 8, 8]\n",
    "            student_attention_downsampled = downsample(student_attention)  # [batch_size, 1, 2, 2]\n",
    "\n",
    "            # Attention transfer loss\n",
    "            attention_transfer_loss = attention_loss(student_attention_downsampled, teacher_attention.detach())\n",
    "\n",
    "            # Classification loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # KD loss\n",
    "            soft_targets = nn.functional.softmax(teacher_logits.detach() / T, dim=-1)  # Detach teacher logits\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "            soft_target_loss = kd_loss(soft_prob, soft_targets) * (T ** 2)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = attention_loss_weight * attention_transfer_loss + ce_loss_weight * label_loss + soft_target_loss_weight * soft_target_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c366d3-f625-455d-9114-a4f7efba563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1059599944087855\n",
      "Epoch 2/10, Loss: 0.955414387728552\n",
      "Epoch 3/10, Loss: 0.8932918386386178\n",
      "Epoch 4/10, Loss: 0.852119562266123\n",
      "Epoch 5/10, Loss: 0.8187829850579772\n",
      "Epoch 6/10, Loss: 0.7870337019491074\n",
      "Epoch 7/10, Loss: 0.7594315499600852\n",
      "Epoch 8/10, Loss: 0.7371146098122268\n",
      "Epoch 9/10, Loss: 0.7146102125992251\n",
      "Epoch 10/10, Loss: 0.6948478000853068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = DeepNN(num_classes=10).to(device)\n",
    "student = LightNN(num_classes=10).to(device)\n",
    "\n",
    "train_attention_transfer(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    train_loader=train_loader,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    attention_loss_weight=0.1,\n",
    "    soft_target_loss_weight=0.2,\n",
    "    ce_loss_weight =0.7,\n",
    "\n",
    "    T=3,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d99a398d-3b6e-45ea-8932-a834de59e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.29%\n",
      "Teacher accuracy: 83.84%\n",
      "Student accuracy without teacher: 72.35%\n",
      "Student accuracy with CE + KD: 74.54%\n",
      "Student accuracy with CE + Grad Attention: 72.50%\n",
      "Student accuracy with CE + Squared Attention: 72.69%\n",
      "Student accuracy with CE +KD + Grad Attention: 73.29%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_light_ce_KD_and_grad = test(student, test_loader, device)\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + Grad Attention: {test_accuracy_light_ce_and_grad:.2f}%\")\n",
    "print(f\"Student accuracy with CE + Squared Attention: {test_accuracy_light_ce_and_feat:.2f}%\")\n",
    "print(f\"Student accuracy with CE +KD + Grad Attention: {test_accuracy_light_ce_KD_and_grad:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c46667-bd61-45f2-84d7-6f25066aa943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacea4b-c8af-4037-8ac6-d10a9f564edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a72d54-e138-4b4d-a978-dc5daaeae00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
